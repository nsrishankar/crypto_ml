{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECE 579M ST: Machine Learning in Cybersecurity\n",
    "### Project Four: Privacy Leakage Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are analyzing location data obtained from the mobile phone of a WPI employee by Google Maps data collection. This is of critical importance as it could be used to estimate future locations of an user from supposedly private information. \n",
    "\n",
    "The provided dataset is collected every 30 minutes over 5 weeks consisting of timestamps, latitude, longitude, accuracy (Google Maps confidence of location), and label (type of location such as home/restaurant etc.). The aim is to train a regression method on the first 4 weeks of data as a training set and use the last week (week 5) as a validation set. The final performance will be tested on an unseen week 6 test set. \n",
    "\n",
    "Since an accuracy metric together with ground-truth labels were provided, we could bring in a notion of **soft labeling** instead of one-hot labeling.\n",
    "\n",
    "Additionally, while I really wanted to utilize a fancy algorithm or even something like Ensemble/XGBoost, maybe just utilizing a simplistic ML algorithm through a better understanding of the data is more apt. :D\n",
    "\n",
    "```\n",
    "Classification Accuracy: 100.0 % from 168 testing datapoints \n",
    "Mean-Squared Error amongst correct classifications: 1.196\n",
    "Mean Predicted Probability is 73.998 % while mean given probability is 67.751 %\n",
    "Predicted Mean probability is greater than the given mean probability. Good tuning.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "## Step 0: Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIST OF ALL IMPORTS\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "# from sklearn.svm import SVR ,NuSVR\n",
    "# from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor, GradientBoostingRegressor\n",
    "\n",
    "from utils import load_data,soft_labeling,sparse2array,argmax_custom,evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Datasets & Basic Exploration of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test and train data files.\n",
      "Loaded map data.\n"
     ]
    }
   ],
   "source": [
    "## DATA PATHS\n",
    "data_path='dataset/'\n",
    "\n",
    "filenames=sorted(glob.glob(data_path+'week*',recursive=True))\n",
    "print(\"Reading test and train data files.\")\n",
    "\n",
    "week1_features,week1_labels=load_data(filenames[0])\n",
    "week2_features,week2_labels=load_data(filenames[1])\n",
    "week3_features,week3_labels=load_data(filenames[2])\n",
    "week4_features,week4_labels=load_data(filenames[3])\n",
    "week5_features,week5_labels=load_data(filenames[4])\n",
    "\n",
    "print(\"Loaded map data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.5       42.372    -71.903      0.94923]\n",
      " [   1.        42.372    -71.906      0.97863]\n",
      " [   1.5       42.373    -71.904      0.91212]\n",
      " ..., \n",
      " [ 167.        43.678    -70.547      0.70833]\n",
      " [ 167.5       43.571    -70.125      0.87538]\n",
      " [ 168.        44.053    -70.582      0.75917]]\n"
     ]
    }
   ],
   "source": [
    "print(week1_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data consists of data from Weeks 1 through 4. Validation data is obtained from Week 5.\n",
      "\n",
      "Week One data has a shape of (336, 4) and been collected over 168.0 hrs.\n",
      "Week Two data has a shape of (336, 4) and been collected over 168.0 hrs.\n",
      "Week Three data has a shape of (336, 4) and been collected over 168.0 hrs.\n",
      "Week Four data has a shape of (336, 4) and been collected over 168.0 hrs.\n",
      "Week Five data has a shape of (336, 4) and been collected over 168.0 hrs.\n",
      "\n",
      "The labels are [48 49 50 51 52] and with frequencies of [756  31 550 326  17].\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data consists of data from Weeks 1 through 4. Validation data is obtained from Week 5.\\n\")\n",
    "print(\"Week One data has a shape of {} and been collected over {} hrs.\".\n",
    "      format(week1_features.shape,week1_features[-1,0]))\n",
    "print(\"Week Two data has a shape of {} and been collected over {} hrs.\".\n",
    "      format(week2_features.shape,week2_features[-1,0]))\n",
    "print(\"Week Three data has a shape of {} and been collected over {} hrs.\".\n",
    "      format(week3_features.shape,week3_features[-1,0]))\n",
    "print(\"Week Four data has a shape of {} and been collected over {} hrs.\".\n",
    "      format(week4_features.shape,week4_features[-1,0]))\n",
    "print(\"Week Five data has a shape of {} and been collected over {} hrs.\".\n",
    "      format(week5_features.shape,week5_features[-1,0]))\n",
    "\n",
    "all_labels=np.vstack((week1_labels,week2_labels,week3_labels,week4_labels,week5_labels))\n",
    "labels_unique,labels_count=np.unique(all_labels,return_counts=True)\n",
    "print(\"\\nThe labels are {} and with frequencies of {}.\".format(labels_unique,labels_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the dataset is unbalanced as some occur relatively infrequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "## Step 2: Dataset Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**\"scikit-multilearn: A scikit-based Python environment for performing multi-label classification\"**_ is a paper and a library by Piotr Szymanski and Tomasz Kajdanowicz of the Wroclaw University of Technology and helps with multi-label classification. Since the features of the dataset consists of timestamps, longitude/latitude, accuracy, and labels [48,49,50,51,52], we can create soft-labels by associating the accuracy with the labels as follows:\n",
    "\n",
    ">`Given-label--> Given Accuracy \n",
    ">Wrong-labels--> (1-Given Accuracy)/(n_classes-1)`\n",
    "\n",
    "\n",
    "If a datapoint has a label of 48, and an accuracy of 94%, we can state the false labels (49,50,51,52) have a probability of (100-94)/4= 1.5% and means that this particular datapoint is of label 48 with accuracy 94%, label 49-52 with accuracy 1.5%. This soft-labeling approximation allows for multi-label classification using a multilearn wrapper around a scikit-learn classifier.\n",
    "\n",
    "Additionally,if timesteps need to be used, an RNN network could be implemented as `RNN==sequential data`, but we could just ignore timesteps and have features of longitude/latitude only and predict based on accumulated data (but might be prone to overfitting). Furthermore, the dataset for each week will be shuffled and forced through k-fold cross-validation to bulk up the number of training-validation datapoints instead of just considering Week 5 alone to be a validation set.\n",
    "\n",
    "A reason against to not use a neural network/RNN is the lack of features, so just simplistic classification will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained final datasets.\n"
     ]
    }
   ],
   "source": [
    "# [48 49 50 51 52]-->[0 1 2 3 4] for soft labels\n",
    "n_classes=5\n",
    "week1_hard_y,week1_soft_y=soft_labeling(week1_features,week1_labels,n_classes)\n",
    "week2_hard_y,week2_soft_y=soft_labeling(week2_features,week2_labels,n_classes)\n",
    "week3_hard_y,week3_soft_y=soft_labeling(week3_features,week3_labels,n_classes)\n",
    "week4_hard_y,week4_soft_y=soft_labeling(week4_features,week4_labels,n_classes)\n",
    "week5_hard_y,week5_soft_y=soft_labeling(week5_features,week5_labels,n_classes)\n",
    "\n",
    "X_aggregate=np.vstack((week1_features[:,1:3],week2_features[:,1:3],week3_features[:,1:3],\n",
    "                       week4_features[:,1:3],week5_features[0:168,1:3])) # Using just latitude and longitude data\n",
    "\n",
    "Y_aggregate=np.vstack((week1_soft_y,week2_soft_y,week3_soft_y,week4_soft_y,week5_soft_y[168:])) # Aggregating soft labels\n",
    "\n",
    "X_test=week5_features[168:,1:3]\n",
    "Y_test_hard=week5_hard_y[168:]\n",
    "Y_test_soft=week5_features[168:,-1] #week5_soft_y\n",
    "\n",
    "\n",
    "X_aggregate_shuffle,Y_aggregate_shuffle=shuffle(X_aggregate,Y_aggregate,random_state=0) # Shuffling dataset\n",
    "# X_test,Y_test=shuffle(X_test,Y_test)\n",
    "\n",
    "print(\"Obtained final datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "## Step 3: Classifier Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Decision Tree- Mean Absolute Error criterion\n",
      "BinaryRelevance(classifier=DecisionTreeRegressor(criterion='mae', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=0, splitter='random'),\n",
      "        require_dense=[False, True]) \n",
      "\n",
      "Fit Time 0.14 s\n",
      "\n",
      "Saving loaded model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trained_DTR.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression Decision Tree- MAE criterion 77.822 %\n",
    "print(\"Regression Decision Tree- Mean Absolute Error criterion\")\n",
    "t0=time.clock()\n",
    "base_classifier=DecisionTreeRegressor(criterion='mae',splitter='random', \n",
    "                                      max_depth=None,min_samples_split=2,\n",
    "                                      random_state=0)\n",
    "\n",
    "classifier = BinaryRelevance(classifier=base_classifier,\n",
    "    require_dense = [False, True])\n",
    "print(classifier,\"\\n\")\n",
    "classifier.fit(X_aggregate_shuffle,Y_aggregate_shuffle)\n",
    "print(\"Fit Time {} s\\n\".format(round(time.clock()-t0,2)))\n",
    "print(\"Saving loaded model.\")\n",
    "joblib.dump(classifier,'trained_DTR.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_proba={}\n",
    "for ind in range(len(X_test)):\n",
    "    predictions=classifier.predict(X_test[ind])\n",
    "    dense_predictions=sparse2array(predictions)\n",
    "\n",
    "    max_index,max_value=argmax_custom(dense_predictions)\n",
    "    test_predictions_proba[ind]=[max_index+48,max_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count,classifier_accuracy,mse_error,mean_pred_probs,mean_true_probs=evaluate(evaluated_dict=test_predictions_proba,\n",
    "                                                     ground_truth_labels=Y_test_hard,\n",
    "                                                     ground_truth_probability=Y_test_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 100.0 % from 168 datapoints \n",
      "Mean-Squared Error amongst correct classifications: 1.196\n",
      "Mean Predicted Probability is 73.998 % while mean given probability is 67.751 %\n",
      "Predicted Mean probability is greater than the given mean probability. Good tuning.\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Accuracy: {} % from {} datapoints \".format(round(100*classifier_accuracy,3),correct_count))\n",
    "print(\"Mean-Squared Error amongst correct classifications: {}\".format(round(100*mse_error,3)))\n",
    "print(\"Mean Predicted Probability is {} % while mean given probability is {} %\".\n",
    "      format(round(100*mean_pred_probs,3),round(100*mean_true_probs,3)))\n",
    "if mean_pred_probs>mean_true_probs:\n",
    "    print(\"Predicted Mean probability is greater than the given mean probability. Good tuning.\")\n",
    "else:\n",
    "    print(\"Predicted Mean probability is less than the given mean probability. More tuning/ different approach needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained Decision Tree Regression Model.\n",
      "Obtained testing features and labels.\n",
      "Obtained predictions\n",
      "335 0.9970238095238095 [ 0.27715753  0.53660399  0.36536064  0.48266871  0.54364654] 0.781931193513 [ 0.40703019  0.0678765   0.29515987  0.16780831  0.06212513]\n",
      "Classification Accuracy: 99.702 % from 336 datapoints \n",
      "Mean Predicted Probability is 78.193%\n"
     ]
    }
   ],
   "source": [
    "from test import test_model\n",
    "saved_model='trained_DTR.pkl'\n",
    "test_features=week1_features\n",
    "test_labels=week1_labels\n",
    "test_predictions_proba=test_model(saved_model,test_features,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Trial Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tried and failed\n",
    "# Adaboost Regressor 75%\n",
    "# print(\"AdaBoost Regressor\")\n",
    "# t0=time.clock()\n",
    "# base_classifier=AdaBoostRegressor(base_estimator=None,n_estimators=100,\n",
    "#                                   learning_rate=1.0,loss='linear',random_state=0)\n",
    "\n",
    "# classifier = BinaryRelevance(classifier=base_classifier,\n",
    "#     require_dense = [False, True])\n",
    "# print(classifier,\"\\n\")\n",
    "# classifier.fit(X_aggregate_shuffle,Y_aggregate_shuffle)\n",
    "# print(\"Fit Time {} s\\n\".format(round(time.clock()-t0,2)))\n",
    "\n",
    "# ExtraTrees Regressor 76%\n",
    "# print(\"ExtraTrees Regressor\")\n",
    "# t0=time.clock()\n",
    "# base_classifier=ExtraTreesRegressor(n_estimators=25,criterion='mse',\n",
    "#                                     max_depth=None,min_samples_split=2,\n",
    "#                                     min_samples_leaf=1,min_weight_fraction_leaf=0.0,\n",
    "#                                     max_features='auto',max_leaf_nodes=None,\n",
    "#                                     min_impurity_decrease=0.0,min_impurity_split=None,\n",
    "#                                     bootstrap=True,oob_score=False,\n",
    "#                                     n_jobs=1,random_state=0,\n",
    "#                                     verbose=1,warm_start=False)\n",
    "\n",
    "# classifier = BinaryRelevance(classifier=base_classifier,\n",
    "#     require_dense = [False, True])\n",
    "# print(classifier,\"\\n\")\n",
    "# classifier.fit(X_aggregate_shuffle,Y_aggregate_shuffle)\n",
    "# print(\"Fit Time {} s\\n\".format(round(time.clock()-t0,2)))\n",
    "\n",
    "# Bagging Regressor 76.286\n",
    "# print(\"Bagging Regressor\")\n",
    "# t0=time.clock()\n",
    "# base_classifier=BaggingRegressor(base_estimator=None,n_estimators=30,\n",
    "#                                     max_samples=1.0,max_features=1.0,\n",
    "#                                     bootstrap=True,bootstrap_features=False,\n",
    "#                                     oob_score=False,warm_start=False,\n",
    "#                                     n_jobs=1,random_state=0,verbose=1)\n",
    "\n",
    "# classifier = BinaryRelevance(classifier=base_classifier,\n",
    "#     require_dense = [False, True])\n",
    "# print(classifier,\"\\n\")\n",
    "# classifier.fit(X_aggregate_shuffle,Y_aggregate_shuffle)\n",
    "# print(\"Fit Time {} s\\n\".format(round(time.clock()-t0,2)))\n",
    "\n",
    "# Bagging Regressor 76.235\n",
    "# print(\"Gradient Boosting Regressor\")\n",
    "# t0=time.clock()\n",
    "# base_classifier=GradientBoostingRegressor(loss='ls',learning_rate=1.0,\n",
    "#                                           n_estimators=20,subsample=1.0,\n",
    "#                                           criterion='friedman_mse',min_samples_split=2,\n",
    "#                                           min_samples_leaf=1,min_weight_fraction_leaf=0.0,\n",
    "#                                           max_depth=3,min_impurity_decrease=0.0,\n",
    "#                                           min_impurity_split=None,init=None,\n",
    "#                                           random_state=0,max_features=None,\n",
    "#                                           alpha=0.95,verbose=1,\n",
    "#                                           max_leaf_nodes=None,warm_start=False,\n",
    "#                                           presort='auto')\n",
    "\n",
    "# classifier = BinaryRelevance(classifier=base_classifier,\n",
    "#     require_dense = [False, True])\n",
    "# print(classifier,\"\\n\")\n",
    "# classifier.fit(X_aggregate_shuffle,Y_aggregate_shuffle)\n",
    "# print(\"Fit Time {} s\\n\".format(round(time.clock()-t0,2)))\n",
    "\n",
    "# SVR 75.078 %\n",
    "# print(\"SVR Regressor\")\n",
    "# t0=time.clock()\n",
    "# base_classifier=SVR(kernel='rbf',degree=3,\n",
    "#                     gamma='auto',coef0=0.0,\n",
    "#                     tol=0.00001,C=1.0,\n",
    "#                     epsilon=0.05,shrinking=True,\n",
    "#                     cache_size=200,verbose=1,max_iter=-1)\n",
    "\n",
    "# classifier = BinaryRelevance(classifier=base_classifier,\n",
    "#     require_dense = [False, True])\n",
    "# print(classifier,\"\\n\")\n",
    "# classifier.fit(X_aggregate_shuffle,Y_aggregate_shuffle)\n",
    "# print(\"Fit Time {} s\\n\".format(round(time.clock()-t0,2)))\n",
    "\n",
    "# NuSVR 76.005 %\n",
    "# print(\"NuSVR Regressor\")\n",
    "# t0=time.clock()\n",
    "# base_classifier=NuSVR(nu=0.95,kernel='rbf',degree=3,\n",
    "#                     gamma='auto',coef0=0.0,\n",
    "#                     tol=0.00001,C=1.0,\n",
    "#                     shrinking=True,\n",
    "#                     cache_size=200,verbose=1,max_iter=-1)\n",
    "\n",
    "# classifier = BinaryRelevance(classifier=base_classifier,\n",
    "#     require_dense = [False, True])\n",
    "# print(classifier,\"\\n\")\n",
    "# classifier.fit(X_aggregate_shuffle,Y_aggregate_shuffle)\n",
    "# print(\"Fit Time {} s\\n\".format(round(time.clock()-t0,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
