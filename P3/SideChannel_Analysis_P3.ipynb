{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECE 579M ST: Machine Learning in Cybersecurity\n",
    "### Project Three: Side-Channel Attack Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement various Machine Learning/Deep Learning techniques (kNN,Decision Tree,SVM,AutoEncoder,CNN,Ensemble,Random Forest,Naive Bayes) etc for the analysis of side-channel data obtained from web broswer profiling.\n",
    "\n",
    "| ML/DL Technique                  | Training Accuracy (%) | Testing Accuracy (%) |\n",
    "| :-                               |:-                     | :-\n",
    "|1. k-Nearest Neighbor             |73.2                   |68.0                  |\n",
    "|----------------------------------|-----------------------|----------------------|\n",
    "|2. Support Vector Classifier      |80.2                   |73.1                  |\n",
    "|----------------------------------|-----------------------|----------------------|\n",
    "|3. Adaboost/Linear SVM            |58.9                   |53.0                  |\n",
    "|----------------------------------|-----------------------|----------------------|\n",
    "|4. Ensemble Voting Classifier     |100.0                  |79.5                  |\n",
    "|----------------------------------|-----------------------|----------------------|\n",
    "|5. Multilayer Perceptron          |100.0                  |78.2                  |\n",
    "|----------------------------------|-----------------------|----------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these methods above, severe hyperparameter tuning was needed which took up a lot of time. Because of the small dataset, the training and test sets were concatenated and run through k-fold cross-validation to generate repeated instances of the data Additionally, the data was normalized so the features have equal weighting, and it was shuffled (as the training labels seemed to be sequential).\n",
    "\n",
    "\n",
    "In fact, all but the fifth algorithm went though 5-folds. The algorithms were k-Nearest Neighbors (ten neighbors), a support vector classifier with an rbf kernel, adaboost algorithm (using 15 weak linear svm classifiers), ensemble soft voting classifier (with three sub classifiers- a random forest classifier, svm classifier, and gradient boosting algorithm). Finally, a simple multi-layer perceptron with 500 hidden layers was used.\n",
    "\n",
    "Use of a Convolution Neural Network/ Autoencoder/LSTM-RNN networks were also looked at, but due to GPU compute constraints weren't pursued much more.\n",
    "\n",
    "*It was seen that a Principal Components Analysis could be performed on the initial dataset to reduce the number of features from 6000 to obtain 1500 latent features ( a much smaller number of features), which would significantly reduce the training time, but slightly increase test/train errors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "## Step 0: Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIST OF ALL IMPORTS\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os.path as path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "from sklearn.model_selection import cross_validate,ShuffleSplit,cross_val_predict,cross_val_score,GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier,VotingClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sknn import ae, mlp\n",
    "\n",
    "random_seed=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Datasets & Basic Exploration of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test and train data files.\n",
      "Loaded side-channel data.\n"
     ]
    }
   ],
   "source": [
    "## DATA PATHS\n",
    "train_data_path='PerfWeb/X_train.dat'\n",
    "train_label_path='PerfWeb/Y_train.dat'\n",
    "test_data_path='PerfWeb/X_test.dat'\n",
    "test_label_path='PerfWeb/Y_test.dat'\n",
    "\n",
    "print(\"Reading test and train data files.\")\n",
    "X_train=np.genfromtxt(train_data_path,delimiter=',')\n",
    "Y_train=np.genfromtxt(train_label_path,delimiter=',').T\n",
    "X_test=np.genfromtxt(test_data_path,delimiter=',')\n",
    "Y_test=np.genfromtxt(test_label_path,delimiter=',').T\n",
    "\n",
    "print(\"Loaded side-channel data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has a shape of (1600, 6000).\n",
      "Training labels has a shape of (1600,).\n",
      "Testing data has a shape of (400, 6000).\n",
      "Testing labels has a shape of (400,).\n",
      "Training data has 6000 features and 1600 data points.\n",
      "Testing data has 6000 features and 400 data points.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data has a shape of {}.\".format(X_train.shape))\n",
    "print(\"Training labels has a shape of {}.\".format(Y_train.shape))\n",
    "print(\"Testing data has a shape of {}.\".format(X_test.shape))\n",
    "print(\"Testing labels has a shape of {}.\".format(Y_test.shape))\n",
    "\n",
    "print(\"Training data has {} features and {} data points.\".format(X_train.shape[1],X_train.shape[0]))\n",
    "print(\"Testing data has {} features and {} data points.\".format(X_test.shape[1],X_test.shape[0]))\n",
    "\n",
    "assert X_train.shape[0]==Y_train.shape[0],'Train data and train labels must have the same shape.'\n",
    "assert X_test.shape[0]==Y_test.shape[0],'Test data and train labels must have the same shape.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of data points is much smaller than the number of features (1600 vs. 6000) AND the the test size is significant compared to the the training data size (almost one fourth) ==> **Concatenate training and testing data and then perform K-Fold cross validation with every training algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape (2000, 6000), Labels Shape (2000, 1)\n",
      "Shuffling data.\n"
     ]
    }
   ],
   "source": [
    "scaler=StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train) \n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "X=np.vstack((X_train,X_test))\n",
    "Y_train=np.reshape(Y_train,(len(Y_train),1))\n",
    "Y_test=np.reshape(Y_test,(len(Y_test),1))\n",
    "Y=np.vstack((Y_train,Y_test))\n",
    "print(\"Data Shape {}, Labels Shape {}\".format(X.shape,Y.shape))\n",
    "print(\"Shuffling data.\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X,Y=shuffle(X,Y,random_state=0)\n",
    "\n",
    "Y=np.reshape(Y,(len(Y),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "## Step 2: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction on features\n",
    "def perform_pca(dataset,n_components=None,perform_pca=True):\n",
    "    if perform_pca:\n",
    "        pca=PCA(n_components=n_components,svd_solver='randomized').fit(dataset)\n",
    "        reduced_dataset=pca.transform(dataset)\n",
    "        return reduced_dataset,pca\n",
    "    else:\n",
    "        return dataset\n",
    "\n",
    "# Find the optimium number of components for dimension reduction using a simple/Linear-SVM classifier\n",
    "def pca_ocheck(X_dataset,y_dataset):\n",
    "    t0_pcacheck=time.time()\n",
    "    pca=PCA(svd_solver='randomized')\n",
    "#     pca=PCA(kernel='linear',random_state=random_seed,n_jobs=-1)\n",
    "    clf=SVC(C=2, kernel='rbf', max_iter=1000,shrinking=True,\n",
    "                random_state=random_seed, tol=0.0001,verbose=0)\n",
    "    \n",
    "    pipeline=Pipeline(steps=[('pca',pca),('svm',clf)])\n",
    "    \n",
    "    n_components=(500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000)\n",
    "   \n",
    "    random_estimator=GridSearchCV(pipeline,dict(pca__n_components=n_components))\n",
    "    random_estimator.fit(X_dataset,y_dataset)\n",
    "    t1_pcacheck=time.time()\n",
    "    print(\"GridSearchCV took {} seconds.\".format(round(t1_pcacheck-t0_pcacheck,3)))\n",
    "    \n",
    "    return (random_estimator.cv_results_,random_estimator.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PCA decomposition to find the best reduced dimensionality representation using grid search cross-validation.\n",
      "Assuming no features have to be normalized as they are all from the same data source.\n",
      "GridSearchCV took 472.937 seconds.\n",
      "Cross-Validation Results: {'split1_train_score': array([0.98888889, 0.96388889, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
      "       0.95833333, 0.95833333]), 'mean_test_score': array([0.673125, 0.685625, 0.686875, 0.686875, 0.686875, 0.686875,\n",
      "       0.686875, 0.686875, 0.686875, 0.686875, 0.686875, 0.686875]), 'split0_train_score': array([0.98557692, 0.95961538, 0.95769231, 0.95769231, 0.95769231,\n",
      "       0.95769231, 0.95769231, 0.95769231, 0.95769231, 0.95769231,\n",
      "       0.95769231, 0.95769231]), 'param_pca__n_components': masked_array(data=[500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500,\n",
      "                   5000, 5500, 6000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'mean_score_time': array([0.42328509, 0.76420546, 0.78511882, 0.78557165, 0.78534317,\n",
      "       0.78552699, 0.78490782, 0.78514695, 0.78510133, 0.78491712,\n",
      "       0.78525654, 0.78527101]), 'mean_fit_time': array([ 3.91731588,  7.76669439,  8.5884215 ,  9.1318419 ,  9.69859743,\n",
      "       10.29908021, 10.93898463, 11.58339874, 12.29201762, 12.9846677 ,\n",
      "       13.72222082, 14.35654958]), 'std_fit_time': array([0.09102473, 0.18885949, 0.33528789, 0.32437046, 0.34375111,\n",
      "       0.33336302, 0.36826801, 0.36252948, 0.36631704, 0.38508791,\n",
      "       0.38650264, 0.38039535]), 'split2_train_score': array([0.99259259, 0.96574074, 0.96018519, 0.96018519, 0.96018519,\n",
      "       0.96018519, 0.96018519, 0.96018519, 0.96018519, 0.96018519,\n",
      "       0.96018519, 0.96018519]), 'split1_test_score': array([0.74230769, 0.74807692, 0.74615385, 0.74615385, 0.74615385,\n",
      "       0.74615385, 0.74615385, 0.74615385, 0.74615385, 0.74615385,\n",
      "       0.74615385, 0.74615385]), 'std_score_time': array([0.00266824, 0.0076798 , 0.01021415, 0.01004354, 0.01074597,\n",
      "       0.0102197 , 0.0103513 , 0.01031794, 0.01031655, 0.01007774,\n",
      "       0.01013547, 0.01057621]), 'std_train_score': array([0.00286562, 0.00256498, 0.00105697, 0.00105697, 0.00105697,\n",
      "       0.00105697, 0.00105697, 0.00105697, 0.00105697, 0.00105697,\n",
      "       0.00105697, 0.00105697]), 'split2_test_score': array([0.70961538, 0.72692308, 0.73076923, 0.73076923, 0.73076923,\n",
      "       0.73076923, 0.73076923, 0.73076923, 0.73076923, 0.73076923,\n",
      "       0.73076923, 0.73076923]), 'split0_test_score': array([0.575     , 0.58928571, 0.59107143, 0.59107143, 0.59107143,\n",
      "       0.59107143, 0.59107143, 0.59107143, 0.59107143, 0.59107143,\n",
      "       0.59107143, 0.59107143]), 'params': [{'pca__n_components': 500}, {'pca__n_components': 1000}, {'pca__n_components': 1500}, {'pca__n_components': 2000}, {'pca__n_components': 2500}, {'pca__n_components': 3000}, {'pca__n_components': 3500}, {'pca__n_components': 4000}, {'pca__n_components': 4500}, {'pca__n_components': 5000}, {'pca__n_components': 5500}, {'pca__n_components': 6000}], 'std_test_score': array([0.07320016, 0.07120616, 0.07057362, 0.07057362, 0.07057362,\n",
      "       0.07057362, 0.07057362, 0.07057362, 0.07057362, 0.07057362,\n",
      "       0.07057362, 0.07057362]), 'mean_train_score': array([0.98901947, 0.96308167, 0.95873694, 0.95873694, 0.95873694,\n",
      "       0.95873694, 0.95873694, 0.95873694, 0.95873694, 0.95873694,\n",
      "       0.95873694, 0.95873694]), 'rank_test_score': array([12, 11,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1], dtype=int32)}\n",
      "Best Estimator: Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=1500, random_state=None,\n",
      "  svd_solver='randomized', tol=0.0, whiten=False)), ('svm', SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=1000, probability=False, random_state=5, shrinking=True,\n",
      "  tol=0.0001, verbose=0))])\n"
     ]
    }
   ],
   "source": [
    "# Applying PCA to obtain reduced dimension representation.\n",
    "\n",
    "print(\"Performing PCA decomposition to find the best reduced dimensionality representation using grid search cross-validation.\")\n",
    "print(\"Assuming no features have to be normalized as they are all from the same data source.\")\n",
    "results,choice=pca_ocheck(X_train,np.reshape(Y_train,(len(Y_train),)))\n",
    "\n",
    "print(\"Cross-Validation Results:\", results)\n",
    "print(\"Best Estimator:\", choice)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Performing PCA decomposition to find the best reduced dimensionality representation using grid search cross-validation.\n",
    "# Assuming no features have to be normalized as they are all from the same data source.\n",
    "# GridSearchCV took 472.937 seconds.\n",
    "# Cross-Validation Results: {'split1_train_score': array([0.98888889, 0.96388889, 0.95833333, 0.95833333, 0.95833333,\n",
    "#        0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
    "#        0.95833333, 0.95833333]), 'mean_test_score': array([0.673125, 0.685625, 0.686875, 0.686875, 0.686875, 0.686875,\n",
    "#        0.686875, 0.686875, 0.686875, 0.686875, 0.686875, 0.686875]), 'split0_train_score': array([0.98557692, 0.95961538, 0.95769231, 0.95769231, 0.95769231,\n",
    "#        0.95769231, 0.95769231, 0.95769231, 0.95769231, 0.95769231,\n",
    "#        0.95769231, 0.95769231]), 'param_pca__n_components': masked_array(data=[500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500,\n",
    "#                    5000, 5500, 6000],\n",
    "#              mask=[False, False, False, False, False, False, False, False,\n",
    "#                    False, False, False, False],\n",
    "#        fill_value='?',\n",
    "#             dtype=object), 'mean_score_time': array([0.42328509, 0.76420546, 0.78511882, 0.78557165, 0.78534317,\n",
    "#        0.78552699, 0.78490782, 0.78514695, 0.78510133, 0.78491712,\n",
    "#        0.78525654, 0.78527101]), 'mean_fit_time': array([ 3.91731588,  7.76669439,  8.5884215 ,  9.1318419 ,  9.69859743,\n",
    "#        10.29908021, 10.93898463, 11.58339874, 12.29201762, 12.9846677 ,\n",
    "#        13.72222082, 14.35654958]), 'std_fit_time': array([0.09102473, 0.18885949, 0.33528789, 0.32437046, 0.34375111,\n",
    "#        0.33336302, 0.36826801, 0.36252948, 0.36631704, 0.38508791,\n",
    "#        0.38650264, 0.38039535]), 'split2_train_score': array([0.99259259, 0.96574074, 0.96018519, 0.96018519, 0.96018519,\n",
    "#        0.96018519, 0.96018519, 0.96018519, 0.96018519, 0.96018519,\n",
    "#        0.96018519, 0.96018519]), 'split1_test_score': array([0.74230769, 0.74807692, 0.74615385, 0.74615385, 0.74615385,\n",
    "#        0.74615385, 0.74615385, 0.74615385, 0.74615385, 0.74615385,\n",
    "#        0.74615385, 0.74615385]), 'std_score_time': array([0.00266824, 0.0076798 , 0.01021415, 0.01004354, 0.01074597,\n",
    "#        0.0102197 , 0.0103513 , 0.01031794, 0.01031655, 0.01007774,\n",
    "#        0.01013547, 0.01057621]), 'std_train_score': array([0.00286562, 0.00256498, 0.00105697, 0.00105697, 0.00105697,\n",
    "#        0.00105697, 0.00105697, 0.00105697, 0.00105697, 0.00105697,\n",
    "#        0.00105697, 0.00105697]), 'split2_test_score': array([0.70961538, 0.72692308, 0.73076923, 0.73076923, 0.73076923,\n",
    "#        0.73076923, 0.73076923, 0.73076923, 0.73076923, 0.73076923,\n",
    "#        0.73076923, 0.73076923]), 'split0_test_score': array([0.575     , 0.58928571, 0.59107143, 0.59107143, 0.59107143,\n",
    "#        0.59107143, 0.59107143, 0.59107143, 0.59107143, 0.59107143,\n",
    "#        0.59107143, 0.59107143]), 'params': [{'pca__n_components': 500}, {'pca__n_components': 1000}, {'pca__n_components': 1500}, {'pca__n_components': 2000}, {'pca__n_components': 2500}, {'pca__n_components': 3000}, {'pca__n_components': 3500}, {'pca__n_components': 4000}, {'pca__n_components': 4500}, {'pca__n_components': 5000}, {'pca__n_components': 5500}, {'pca__n_components': 6000}], 'std_test_score': array([0.07320016, 0.07120616, 0.07057362, 0.07057362, 0.07057362,\n",
    "#        0.07057362, 0.07057362, 0.07057362, 0.07057362, 0.07057362,\n",
    "#        0.07057362, 0.07057362]), 'mean_train_score': array([0.98901947, 0.96308167, 0.95873694, 0.95873694, 0.95873694,\n",
    "#        0.95873694, 0.95873694, 0.95873694, 0.95873694, 0.95873694,\n",
    "#        0.95873694, 0.95873694]), 'rank_test_score': array([12, 11,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1], dtype=int32)}\n",
    "# Best Estimator: Pipeline(memory=None,\n",
    "#      steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=1500, random_state=None,\n",
    "#   svd_solver='randomized', tol=0.0, whiten=False)), ('svm', SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "#   max_iter=1000, probability=False, random_state=5, shrinking=True,\n",
    "#   tol=0.0001, verbose=0))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above PCA analysis shows that the train dataset and labels can be reduced from a dimension of [n_examples x n_features] where n_features is 6000 to [n_examples x n_reduced] where n_reduced is 1500. PCA simplifies the dataset to have latent features with maximum variance (with some loss in data), which would make training faster however, will result in slightly lower accuracies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.1: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit_time', 'score_time', 'test_score', 'train_score']\n",
      "Scores {'score_time': array([9.63397121, 9.60387492, 9.73712039, 9.67707562, 9.81315112,\n",
      "       9.66166234, 9.58163428, 9.65785241, 9.60549831, 9.8603456 ,\n",
      "       9.61689854, 9.26228714, 9.2852025 , 9.76329279, 9.66433597,\n",
      "       9.55012488, 9.75235558, 9.56144786, 9.28945518, 9.52209425]), 'fit_time': array([0.3906846 , 0.80480051, 0.71776509, 0.7332921 , 0.70535374,\n",
      "       0.66731048, 0.60951853, 0.6636796 , 0.67183137, 0.67307115,\n",
      "       0.71574068, 0.67207074, 0.6440208 , 0.55777931, 0.67455888,\n",
      "       0.6189394 , 0.65745544, 0.73351693, 0.57717037, 0.58255339]), 'train_score': array([0.74125 , 0.735   , 0.73    , 0.736875, 0.731875, 0.734375,\n",
      "       0.7325  , 0.735   , 0.731875, 0.73125 , 0.7275  , 0.72625 ,\n",
      "       0.72625 , 0.731875, 0.73375 , 0.72875 , 0.73375 , 0.728125,\n",
      "       0.733125, 0.735   ]), 'test_score': array([0.6725, 0.6775, 0.6875, 0.67  , 0.6925, 0.6925, 0.67  , 0.705 ,\n",
      "       0.6725, 0.655 , 0.6875, 0.725 , 0.705 , 0.68  , 0.68  , 0.6825,\n",
      "       0.6625, 0.64  , 0.6675, 0.6825])}\n",
      "Training accuracy: 0.732 (+/- 0.007)\n",
      "Testing accuracy: 0.68 (+/- 0.037)\n"
     ]
    }
   ],
   "source": [
    "n_splits=20\n",
    "\n",
    "knn_clf=KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "cross_validate_shuffle=ShuffleSplit(n_splits=n_splits,test_size=0.2,random_state=random_seed)\n",
    "knn_scores=cross_validate(knn_clf,X,Y,cv=cross_validate_shuffle,scoring='accuracy',return_train_score=True,n_jobs=-1)\n",
    "\n",
    "print(sorted(knn_scores.keys()))\n",
    "print(\"Scores\",knn_scores)\n",
    "\n",
    "train_accuracy_array=knn_scores['train_score']\n",
    "test_accuracy_array=knn_scores['test_score']\n",
    "\n",
    "print(\"Training accuracy: {} (+/- {})\" .format(round(train_accuracy_array.mean(),3),\n",
    "                                               round(train_accuracy_array.std()*2,3)))\n",
    "\n",
    "print(\"Testing accuracy: {} (+/- {})\" .format(round(test_accuracy_array.mean(),3),\n",
    "                                               round(test_accuracy_array.std()*2,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit_time', 'score_time', 'test_score', 'train_score']\n",
      "Scores {'score_time': array([6.38561392, 6.48737097, 6.41299987, 6.42047834, 4.48027539]), 'fit_time': array([11.85021877, 12.02202535, 11.92711115, 11.75989532, 11.14598298]), 'train_score': array([0.81    , 0.801875, 0.804375, 0.795   , 0.80125 ]), 'test_score': array([0.72  , 0.7175, 0.735 , 0.7375, 0.745 ])}\n",
      "Training accuracy: 0.802 (+/- 0.01)\n",
      "Testing accuracy: 0.731 (+/- 0.021)\n"
     ]
    }
   ],
   "source": [
    "# linear_svc_clf=LinearSVC(C=1.0, loss='squared_hinge', max_iter=1000, dual=True, \n",
    "#                 penalty='l2', random_state=random_seed, tol=0.0001,verbose=0)\n",
    "\n",
    "svc_clf=SVC(C=2, kernel='rbf', max_iter=1000,shrinking=True,\n",
    "                random_state=random_seed, tol=0.0001,verbose=1) # 2,poly good 2, rbf better\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "\n",
    "cross_validate_shuffle=ShuffleSplit(n_splits=n_splits,test_size=0.2,random_state=random_seed)\n",
    "svm_scores=cross_validate(svc_clf,X,Y,cv=cross_validate_shuffle,scoring='accuracy',\n",
    "                          return_train_score=True,n_jobs=-1)\n",
    "\n",
    "print(sorted(svm_scores.keys()))\n",
    "print(\"Scores\",svm_scores)\n",
    "\n",
    "train_accuracy_array=svm_scores['train_score']\n",
    "test_accuracy_array=svm_scores['test_score']\n",
    "\n",
    "print(\"Training accuracy: {} (+/- {})\" .format(round(train_accuracy_array.mean(),3),\n",
    "                                               round(train_accuracy_array.std()*2,3)))\n",
    "\n",
    "print(\"Testing accuracy: {} (+/- {})\" .format(round(test_accuracy_array.mean(),3),\n",
    "                                               round(test_accuracy_array.std()*2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3: SVM/ Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit_time', 'score_time', 'test_score', 'train_score']\n",
      "Scores {'score_time': array([80.47567606, 80.81861782, 80.76564455, 69.78369212, 60.25447512]), 'fit_time': array([1262.34320617, 1232.52710223, 1232.32786369, 1176.02218604,\n",
      "       1165.50443244]), 'train_score': array([0.578     , 0.60533333, 0.59333333, 0.56733333, 0.6       ]), 'test_score': array([0.54 , 0.534, 0.542, 0.518, 0.514])}\n",
      "Training accuracy: 0.589 (+/- 0.028)\n",
      "Testing accuracy: 0.53 (+/- 0.023)\n"
     ]
    }
   ],
   "source": [
    "# Without K-Fold cross-validation (results in extremely low test/train accuracy)\n",
    "# With K-Fold cross-validation, still low values\n",
    "n_splits=5\n",
    "\n",
    "Y_train=np.reshape(Y_train,(len(Y_train),))\n",
    "\n",
    "adaboost_clf=AdaBoostClassifier(SVC(probability=True,kernel='linear'),\n",
    "                                n_estimators=10,learning_rate=2,algorithm='SAMME.R',random_state=0)\n",
    "\n",
    "cross_validate_shuffle=ShuffleSplit(n_splits=n_splits,test_size=0.25,random_state=random_seed)\n",
    "adaboost_scores=cross_validate(adaboost_clf,X,Y,cv=cross_validate_shuffle,scoring='accuracy',return_train_score=True,n_jobs=-1)\n",
    "\n",
    "print(sorted(adaboost_scores.keys()))\n",
    "print(\"Scores\",adaboost_scores)\n",
    "\n",
    "train_accuracy_array=adaboost_scores['train_score']\n",
    "test_accuracy_array=adaboost_scores['test_score']\n",
    "\n",
    "print(\"Training accuracy: {} (+/- {})\" .format(round(train_accuracy_array.mean(),3),\n",
    "                                               round(train_accuracy_array.std()*2,3)))\n",
    "\n",
    "print(\"Testing accuracy: {} (+/- {})\" .format(round(test_accuracy_array.mean(),3),\n",
    "                                               round(test_accuracy_array.std()*2,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.4: Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "voting_clf_1=RandomForestClassifier(n_estimators=15,criterion='gini', \n",
    "                                     max_depth=None,min_samples_split=2,\n",
    "                                     min_samples_leaf=2,max_features='auto',max_leaf_nodes=None,\n",
    "                                     bootstrap=True,n_jobs=-1,\n",
    "                                     random_state=random_seed,warm_start=True, class_weight=None)\n",
    "\n",
    "voting_clf_2=SVC(C=1, kernel='rbf', max_iter=1000,shrinking=True,probability=True,\n",
    "                random_state=random_seed, tol=0.0001,verbose=1)\n",
    "\n",
    "voting_clf_3=GradientBoostingClassifier(learning_rate=0.1,n_estimators=100,\n",
    "                                        subsample=1.0,criterion='friedman_mse',max_depth=4, \n",
    "                                        max_leaf_nodes=None,presort='auto')\n",
    "\n",
    "estimators=[('randforest',voting_clf_1),('svm',voting_clf_2),('gradientboost',voting_clf_3)]                                    \n",
    "\n",
    "voting_classifier=VotingClassifier(estimators=estimators,voting='soft',flatten_transform=True)\n",
    "\n",
    "voting_classifier=voting_classifier.fit(X_train,Y_train)\n",
    "Y_train_predict=voting_classifier.predict(X_train)\n",
    "Y_test_predict=voting_classifier.predict(X_test)   \n",
    "\n",
    "train_accuracy=accuracy_score(Y_train,Y_train_predict)\n",
    "test_accuracy=accuracy_score(Y_test,Y_test_predict)\n",
    "\n",
    "print(\"Training accuracy: {}\" .format(round(train_accuracy,3)))\n",
    "\n",
    "print(\"Testing accuracy: {}\" .format(round(test_accuracy,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.5: MultiLayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "mlp_clf=MLPClassifier(hidden_layer_sizes=(500,),activation='relu',solver='lbfgs',\n",
    "                      alpha=0.0001,batch_size='auto',learning_rate='adaptive', \n",
    "                      learning_rate_init=0.001,max_iter=300, \n",
    "                      shuffle=True,random_state=random_seed,tol=0.00001, \n",
    "                      verbose=True,warm_start=True,momentum=0.9,\n",
    "                      nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,\n",
    "                      epsilon=1e-08)\n",
    "\n",
    "mlp_clf=mlp_clf.fit(X_train,Y_train)\n",
    "\n",
    "Y_train_predict=mlp_clf.predict(X_train)\n",
    "Y_test_predict=mlp_clf.predict(X_test)   \n",
    "\n",
    "train_accuracy=accuracy_score(Y_train,Y_train_predict)\n",
    "test_accuracy=accuracy_score(Y_test,Y_test_predict)\n",
    "\n",
    "print(\"Training accuracy: {}\" .format(round(train_accuracy,3)))\n",
    "\n",
    "print(\"Testing accuracy: {}\" .format(round(test_accuracy,3)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mlp_model():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(500,input_dim=6000,activation='relu'))\n",
    "    model.add(Dense(200,activation='relu'))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(40,activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "mlp_model=KerasClassifier(build_fn=mlp_model,epochs=200,batch_size=25,verbose=1)\n",
    "cv_mlp=StratifiedKFold(n_splits=10,shuffle=True,random_state=random_seed)\n",
    "# mlp_scores=cross_validate(mlp_model,X,Y,cv=cv_mlp,scoring='accuracy',return_train_score=True,n_jobs=-1)\n",
    "mlp_scores=cross_val_score(mlp_model,X,Y,cv=cv_mlp)\n",
    "print(sorted(mlp_scores.keys()))\n",
    "print(\"Scores\",mlp_scores)\n",
    "\n",
    "train_accuracy_array=mlp_scores['train_score']\n",
    "test_accuracy_array=mlp_scores['test_score']\n",
    "\n",
    "print(\"Training accuracy: {} (+/- {})\" .format(round(train_accuracy_array.mean(),3),\n",
    "                                               round(train_accuracy_array.std()*2,3)))\n",
    "\n",
    "print(\"Testing accuracy: {} (+/- {})\" .format(round(test_accuracy_array.mean(),3),\n",
    "                                               round(test_accuracy_array.std()*2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.6: Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "X_train_RNN=X_train.reshape(X_train.shape[0],-1,1)\n",
    "X_test_RNN=X_test.reshape(X_test.shape[0],-1,1)\n",
    "X_train_RNN=X_train_RNN.astype('int32')\n",
    "X_test_RNN=X_test_RNN.astype('int32')\n",
    "\n",
    "Y_train_cat=np_utils.to_categorical(Y_train)\n",
    "Y_test_cat=np_utils.to_categorical(Y_test)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(SimpleRNN(10,\n",
    "#                     kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "#                     recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "#                     activation='relu',input_shape=X_train_RNN.shape[1:]))\n",
    "\n",
    "model.add(LSTM(200, input_shape=X_train_RNN.shape[1:],activation='tanh', inner_activation='sigmoid'))\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('softmax'))\n",
    "rmsprop = RMSprop(clipnorm=clip_norm)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n",
    "\n",
    "model.fit(X_train_RNN,Y_train_cat,batch_size=25,nb_epoch=150,\n",
    "          show_accuracy=True, verbose=1, validation_data=(X_test_RNN,Y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.7: AutoEncoder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_autoencoder=ae.AutoEncoder(layers=[ae.Layer(\"Tanh\",units=256),\n",
    "                                        ae.Layer(\"Sigmoid\",units=80)],\n",
    "                                 learning_rate=0.0015,n_iter=20)\n",
    "model_autoencoder.fit(X)\n",
    "\n",
    "model_mlp=mlp.Classifier(layers=[mlp.Layer(\"Tanh\",units=256),\n",
    "                                mlp.Layer(\"Sigmoid\",units=80),\n",
    "                                mlp.Layer(\"Sigmoid\",units=40)])\n",
    "model_autoencoder.transfer(model_mlp)\n",
    "model_mlp.fit(X_train,Y_train)\n",
    "\n",
    "Y_train_predict=model_mlp.predict(X_train)\n",
    "Y_test_predict=model_mlp.predict(X_test)   \n",
    "\n",
    "train_accuracy=accuracy_score(Y_train,Y_train_predict)\n",
    "test_accuracy=accuracy_score(Y_test,Y_test_predict)\n",
    "\n",
    "print(\"Training accuracy: {}\" .format(round(train_accuracy,3)))\n",
    "\n",
    "print(\"Testing accuracy: {}\" .format(round(test_accuracy,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
